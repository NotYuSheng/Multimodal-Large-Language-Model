Project Scope
=============

This section documents the development of the localized Multimodal Large Language Model (MLLM) integrated with Streamlit and Ollama for text and image processing tasks.

Code and further instructions on how to deploy can be found on the GitHub repository:
`NotYuSheng/Multimodal-Large-Language-Model <https://github.com/NotYuSheng/Multimodal-Large-Language-Model>`_.

1.1 Overview
------------

The project aims to build a Multimodal LLM web application that can take image, video, audio, and text inputs, and answer questions pertaining to them. 

- **Domain**: Counter-Terrorism Intelligence
- **Models**: Pre-trained models
- **Environment**: Offline production

.. image:: path/to/streamlit_web_interface_image.png
   :alt: Streamlit web interface

.. _project_features:
